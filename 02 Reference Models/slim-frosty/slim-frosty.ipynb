{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "import numpy as np \n",
    "\n",
    "import scipy.stats as sts\n",
    "\n",
    "import tensorflow as tf\n",
    "#tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "frosty_tokenizer = Tokenizer()\n",
    "frosty_data = open('robert_frost.txt').read()\n",
    "\n",
    "slim_tokenizer = Tokenizer()\n",
    "slim_data = open('robert_frost.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_preparation(data, tokenizer):\n",
    "\n",
    "    # basic cleanup\n",
    "    corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "    # tokenization\t\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "    # create input sequences using list of tokens\n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "\n",
    "    # pad sequences \n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "    # create predictors and label\n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "\n",
    "    return predictors, label, max_sequence_len, total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim_predictors, slim_label, slim_max_sequence_len, slim_total_words = dataset_preparation(slim_data,slim_tokenizer)\n",
    "frosty_predictors, frosty_label, frosty_max_sequence_len, frosty_total_words = dataset_preparation(frosty_data,frosty_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(predictors, label, max_sequence_len, total_words):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))\n",
    "    model.add(LSTM(150, return_sequences = True))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "    model.fit(predictors, label, epochs=100, verbose=1, callbacks=[earlystop])\n",
    "    print(model.summary())\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "frosty_model = create_model(frosty_predictors,\n",
    "                            frosty_label, \n",
    "                            frosty_max_sequence_len, \n",
    "                            frosty_total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim_model = create_model(slim_predictors, \n",
    "                          slim_label, \n",
    "                          slim_max_sequence_len, \n",
    "                          slim_total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_text, next_words, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "# List of all the state variables in the system and their initial values\n",
    "\n",
    "prompt_string = \"Double down divergent road rap\"\n",
    "\n",
    "initial_conditions = {\n",
    "    'transcript': prompt_string, # collaborative text\n",
    "    'slim_says' : \"\",\n",
    "    'frost_says': \"\",\n",
    "    'slim': (0.0,0.0, 0.0), #slim's opinions: scores of the suggestions, (fullprior,+slim,+frosty) \n",
    "    'frosty':(0.0,0.0, 0.0) #frosty opinions: scores of the suggestions, (fullprior,+slim,+frosty) \n",
    "}\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extra utility, may not be needed\n",
    "def new_line(transcript, max_sequence_len=12):\n",
    "    \n",
    "    current_line = transcript.split('\\n')[-1]\n",
    "    len_current_line = current_line.split(' ')\n",
    "    \n",
    "    val = bool(len_current_line>max_sequence_len)\n",
    "    \n",
    "    return val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slim_suggestions(params, step, sL, s):\n",
    "    \n",
    "    #user model evaluate\n",
    "    transcript = s['transcript']\n",
    "    current_line = transcript.split('\\n')[-1]\n",
    "    \n",
    "    lam = 3\n",
    "    rv = sts.poisson.rvs(lam)\n",
    "    \n",
    "    string = generate_text(slim_model, current_line, rv, frosty_max_sequence_len)\n",
    "    \n",
    "    #higher should be better\n",
    "    return {'slim_says': string}\n",
    "\n",
    "def get_frosty_suggestion(params, step, sL, s):\n",
    "    \n",
    "    #user model Generate:\n",
    "    transcript = s['transcript']\n",
    "    current_line = transcript.split('\\n')[-1]\n",
    "    \n",
    "    lam = 3\n",
    "    rv = sts.poisson.rvs(lam)\n",
    "    \n",
    "    string = generate_text(frosty_model,current_line, rv, frosty_max_sequence_len)\n",
    "    \n",
    "    #higher should be better\n",
    "    return {'frosty_says': string}\n",
    "\n",
    "def store_slim_suggestion(params, step, sL, s, _input):\n",
    "    \n",
    "    y = 'slim_says'\n",
    "    x = _input['slim_says']\n",
    "    \n",
    "    return(y, x)\n",
    "\n",
    "def store_frosty_suggestion(params, step, sL, s, _input):\n",
    "    \n",
    "    y = 'frosty_says'\n",
    "    x = _input['frosty_says']\n",
    "    \n",
    "    return(y, x) \n",
    "\n",
    "def collaborate(params, step, sL, s):\n",
    "    \n",
    "    #use the suggestions and compute opinions\n",
    "    slim_says = s['slim_says']\n",
    "    frosty_says = s['frost_says']\n",
    "    prior_transcript = s['transcript']    \n",
    "    \n",
    "    #some function of evaluate methods from the RNNS\n",
    "    \n",
    "    #evaluates the existing script\n",
    "    slim_opinion_prior_transcript = get_slim_opinion(prior_transcript, \"\")\n",
    "    frosty_opinion_prior_transcript = get_frosty_opinion(prior_transcript, \"\")\n",
    "    \n",
    "    #evaluates the scipts with the new words\n",
    "    frosty_opinion_frosty_says =get_frosty_opinion(prior_transcript, frosty_says)\n",
    "    slim_opinion_frosty_says = get_slim_opinion(prior_transcript, frosty_says)\n",
    "    slim_opinion_slim_says = get_slim_opinion(prior_transcript, slim_says)\n",
    "    frosty_opinion_slim_says = get_frosty_opinion(prior_transcript, slim_says)\n",
    "    \n",
    "    opinion_dict = {\n",
    "                    'slim':(slim_opinion_prior_transcript,\n",
    "                            lim_opinion_slim_says,\n",
    "                            slim_opinion_frosty_says), \n",
    "                    'frosty':(frosty_opinion_prior_transcript,\n",
    "                             frosty_opinion_slim_says,\n",
    "                             frosty_opinion_frosty_says)\n",
    "                   }\n",
    "    \n",
    "    return opinion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slim_opinion(transcript, suggested_word):\n",
    "\n",
    "    #use slim_model evaluate\n",
    "    \n",
    "    #higher should be better\n",
    "    return score\n",
    "\n",
    "def get_frosty_opinion(transcript, suggested_word):\n",
    "    \n",
    "    #use frosty_model evaluate\n",
    "    \n",
    "    #higher should be better\n",
    "    return score\n",
    "\n",
    "def choose(transcript, suggestions, opinions):\n",
    "    \n",
    "    #suggestions (slim, frosty) and opinions (slim, frosty)\n",
    "    \n",
    "    #heurstic funciton of the opinions returns 0 or 1\n",
    "    choice = 0 #or 1\n",
    "\n",
    "    return suggestions[choice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_transcript(params, step, sL, s, _input):\n",
    "    \n",
    "    prior_transcript = s['transcript']\n",
    "    opinions = (_input['slim'],  _input['frosty'])\n",
    "    suggestions = (s['slim_says'], s['frosty_says'])\n",
    "    \n",
    "    y = 'transcript'\n",
    "    \n",
    "    x = choose(prior_transcript, suggestions, opinions)\n",
    "    \n",
    "    return (y, x)\n",
    "\n",
    "def store_slim_opinion(params, step, sL, s, _input):\n",
    "    \n",
    "    opinion = _input['slim']\n",
    "    \n",
    "    y = 'slim'\n",
    "    \n",
    "    x = opinion\n",
    "    \n",
    "    return (y, x)\n",
    "\n",
    "def store_frosty_opinion(params, step, sL, s, _input):\n",
    "    \n",
    "    opinion = _input['frosty']\n",
    "    \n",
    "    y = 'frosty'\n",
    "    \n",
    "    x = opinion\n",
    "    \n",
    "    return (y, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "# In the Partial State Update Blocks, the user specifies if state update functions will be run in series or in parallel\n",
    "partial_state_update_blocks = [\n",
    "    { \n",
    "        'policies': {\n",
    "            'slim':get_slim_suggestion,\n",
    "            'frosty':get_frosty_suggestion \n",
    "        },\n",
    "        'variables': { # The following state variables will be updated simultaneously\n",
    "            'slim_says': store_slim_suggestion,\n",
    "            'frosty_says': store_frosty_suggestion\n",
    "        }\n",
    "    },\n",
    "    { \n",
    "        'policies': {\n",
    "            'collaborate': collaborate\n",
    "        },\n",
    "        'variables': { # The following state variables will be updated simultaneously\n",
    "            'transcript': update_transcript,\n",
    "            'slim':store_slim_opinions,\n",
    "            'frosty': store_frosty_opinions\n",
    "        }\n",
    "    }\n",
    "]\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Configuration Parameters\n",
    "Lastly, we define the number of timesteps and the number of Monte Carlo runs of the simulation. These parameters must be passed in a dictionary, in `dict_keys` `T` and `N`, respectively. In our example, we'll run the simulation for 10 timesteps. And because we are dealing with a deterministic system, it makes no sense to have multiple Monte Carlo runs, so we set `N=1`. We'll ignore the `M` key for now and set it to an empty `dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "# Settings of general simulation parameters, unrelated to the system itself\n",
    "# `T` is a range with the number of discrete units of time the simulation will run for;\n",
    "# `N` is the number of times the simulation will be run (Monte Carlo runs)\n",
    "# In this example, we'll run the simulation once (N=1) and its duration will be of 10 timesteps\n",
    "# We'll cover the `M` key in a future article. For now, let's leave it empty\n",
    "simulation_parameters = {\n",
    "    'T': range(10),\n",
    "    'N': 1,\n",
    "    'M': {}\n",
    "}\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "We have defined the state variables of our system and their initial conditions, as well as the state update functions, which have been grouped in a single state update block. We have also specified the parameters of the simulation (number of timesteps and runs). We are now ready to put all those pieces together in a `Configuration` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cadCAD.configuration import Configuration\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "# The configurations above are then packaged into a `Configuration` object\n",
    "config = Configuration(initial_state=initial_conditions, #dict containing variable names and initial values\n",
    "                       partial_state_update_blocks=partial_state_update_blocks, #dict containing state update functions\n",
    "                       sim_config=simulation_parameters #dict containing simulation parameters\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the engine\n",
    "We are now ready to run the engine with the configuration defined above. Instantiate an ExecutionMode, an ExecutionContext and an Executor objects, passing the Configuration object to the latter. Then run the `main()` method of the Executor object, which returns the results of the experiment in the first element of a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from cadCAD.engine import ExecutionMode, ExecutionContext, Executor\n",
    "exec_mode = ExecutionMode()\n",
    "exec_context = ExecutionContext(exec_mode.single_proc)\n",
    "executor = Executor(exec_context, [config]) # Pass the configuration object inside an array\n",
    "raw_result, tensor = executor.main() # The `main()` method returns a tuple; its first elements contains the raw results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the results\n",
    "We can now convert the raw results into a DataFrame for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(raw_result)\n",
    "df.set_index(['run', 'timestep', 'substep'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_About BlockScience_  \n",
    "[BlockScience](http://bit.ly/github_articles_M_1) is a research and engineering firm specialized in complex adaptive systems and applying practical methodologies from engineering design, development and testing to projects in emerging technologies such as blockchain. Follow us on [Medium](http://bit.ly/bsci-medium) or [Twitter](http://bit.ly/bsci-twitter) to stay in touch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
