{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "import numpy as np \n",
    "\n",
    "import scipy.stats as sts\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "frosty_tokenizer = Tokenizer()\n",
    "frosty_data = open('robert_frost.txt').read()\n",
    "\n",
    "slim_tokenizer = Tokenizer()\n",
    "slim_data = open('slim_shady.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_preparation(data, tokenizer):\n",
    "\n",
    "    # basic cleanup\n",
    "    corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "    # tokenization\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "    # create input sequences using list of tokens\n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "\n",
    "    # pad sequences \n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "    # create predictors and label\n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "\n",
    "    return predictors, label, max_sequence_len, total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim_predictors, slim_label, slim_max_sequence_len, slim_total_words = dataset_preparation(slim_data,slim_tokenizer)\n",
    "frosty_predictors, frosty_label, frosty_max_sequence_len, frosty_total_words = dataset_preparation(frosty_data,frosty_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(predictors, label, max_sequence_len, total_words):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))\n",
    "    model.add(LSTM(150, return_sequences = True))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "    model.fit(predictors, label, epochs=100, verbose=1, callbacks=[earlystop])\n",
    "    print(model.summary())\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aclarkdata/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/aclarkdata/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 6.9501 - acc: 0.0458\n",
      "Epoch 2/100\n",
      " 160/9519 [..............................] - ETA: 9s - loss: 6.3138 - acc: 0.0375   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aclarkdata/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:569: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9519/9519 [==============================] - 9s 969us/step - loss: 6.5365 - acc: 0.0481\n",
      "Epoch 3/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 6.5091 - acc: 0.0486\n",
      "Epoch 4/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 6.5084 - acc: 0.0462\n",
      "Epoch 5/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 6.5067 - acc: 0.0471\n",
      "Epoch 6/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 6.5060 - acc: 0.0478\n",
      "Epoch 7/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 6.5060 - acc: 0.0474\n",
      "Epoch 8/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 6.5108 - acc: 0.0466\n",
      "Epoch 9/100\n",
      "9519/9519 [==============================] - 12s 1ms/step - loss: 6.5050 - acc: 0.0471\n",
      "Epoch 10/100\n",
      "9519/9519 [==============================] - 11s 1ms/step - loss: 6.5078 - acc: 0.0480\n",
      "Epoch 11/100\n",
      "9519/9519 [==============================] - 7s 735us/step - loss: 6.5077 - acc: 0.0481\n",
      "Epoch 12/100\n",
      "9519/9519 [==============================] - 6s 587us/step - loss: 6.5053 - acc: 0.0467\n",
      "Epoch 13/100\n",
      "9519/9519 [==============================] - 7s 738us/step - loss: 6.5086 - acc: 0.0486\n",
      "Epoch 14/100\n",
      "9519/9519 [==============================] - 7s 787us/step - loss: 6.5086 - acc: 0.0474\n",
      "Epoch 15/100\n",
      "9519/9519 [==============================] - 6s 653us/step - loss: 6.5069 - acc: 0.0486\n",
      "Epoch 16/100\n",
      "9519/9519 [==============================] - 6s 581us/step - loss: 6.5066 - acc: 0.0477\n",
      "Epoch 17/100\n",
      "9519/9519 [==============================] - 7s 701us/step - loss: 6.5019 - acc: 0.0486\n",
      "Epoch 18/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 6.5036 - acc: 0.0478\n",
      "Epoch 19/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 6.5066 - acc: 0.0486\n",
      "Epoch 20/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 6.4959 - acc: 0.0486\n",
      "Epoch 21/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 6.4842 - acc: 0.0470\n",
      "Epoch 22/100\n",
      "9519/9519 [==============================] - 11s 1ms/step - loss: 6.4768 - acc: 0.0486\n",
      "Epoch 23/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 6.4739 - acc: 0.0481\n",
      "Epoch 24/100\n",
      "9519/9519 [==============================] - 8s 879us/step - loss: 6.4717 - acc: 0.0486\n",
      "Epoch 25/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 6.4694 - acc: 0.0473\n",
      "Epoch 26/100\n",
      "9519/9519 [==============================] - 11s 1ms/step - loss: 6.4748 - acc: 0.0486\n",
      "Epoch 27/100\n",
      "9519/9519 [==============================] - 11s 1ms/step - loss: 6.4697 - acc: 0.0471\n",
      "Epoch 28/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 6.4686 - acc: 0.0486\n",
      "Epoch 29/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 6.4702 - acc: 0.0486\n",
      "Epoch 30/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 6.4689 - acc: 0.0486\n",
      "Epoch 31/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 6.4672 - acc: 0.0486\n",
      "Epoch 32/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 6.4612 - acc: 0.0486\n",
      "Epoch 33/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 6.4637 - acc: 0.0486\n",
      "Epoch 34/100\n",
      "9519/9519 [==============================] - 11s 1ms/step - loss: 6.4650 - acc: 0.0471\n",
      "Epoch 35/100\n",
      "9519/9519 [==============================] - 12s 1ms/step - loss: 6.4660 - acc: 0.0486\n",
      "Epoch 36/100\n",
      "9519/9519 [==============================] - 9s 962us/step - loss: 6.4643 - acc: 0.0486\n",
      "Epoch 37/100\n",
      "9519/9519 [==============================] - 7s 773us/step - loss: 6.4501 - acc: 0.0479\n",
      "Epoch 38/100\n",
      "9519/9519 [==============================] - 5s 566us/step - loss: 6.4449 - acc: 0.0486\n",
      "Epoch 39/100\n",
      "9519/9519 [==============================] - 5s 552us/step - loss: 6.4612 - acc: 0.0486\n",
      "Epoch 40/100\n",
      "9519/9519 [==============================] - 8s 850us/step - loss: 6.4362 - acc: 0.0486\n",
      "Epoch 41/100\n",
      "9519/9519 [==============================] - 11s 1ms/step - loss: 6.4369 - acc: 0.0486\n",
      "Epoch 42/100\n",
      "9519/9519 [==============================] - 11s 1ms/step - loss: 6.4348 - acc: 0.0486\n",
      "Epoch 43/100\n",
      "9519/9519 [==============================] - 13s 1ms/step - loss: 6.4327 - acc: 0.0486\n",
      "Epoch 44/100\n",
      "9519/9519 [==============================] - 13s 1ms/step - loss: 6.4270 - acc: 0.0486\n",
      "Epoch 45/100\n",
      "9519/9519 [==============================] - 14s 1ms/step - loss: 6.4297 - acc: 0.0486\n",
      "Epoch 46/100\n",
      "9519/9519 [==============================] - 13s 1ms/step - loss: 6.4274 - acc: 0.0486\n",
      "Epoch 47/100\n",
      "9519/9519 [==============================] - 8s 811us/step - loss: 6.4270 - acc: 0.0486\n",
      "Epoch 48/100\n",
      "9519/9519 [==============================] - 14s 1ms/step - loss: 6.4243 - acc: 0.0486\n",
      "Epoch 49/100\n",
      "9519/9519 [==============================] - 17s 2ms/step - loss: 6.4375 - acc: 0.0486\n",
      "Epoch 50/100\n",
      "9519/9519 [==============================] - 14s 1ms/step - loss: 6.4100 - acc: 0.0480\n",
      "Epoch 51/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 6.3577 - acc: 0.0486\n",
      "Epoch 52/100\n",
      "9519/9519 [==============================] - 11s 1ms/step - loss: 6.2411 - acc: 0.0519\n",
      "Epoch 53/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 6.0788 - acc: 0.0574\n",
      "Epoch 54/100\n",
      "9519/9519 [==============================] - 8s 790us/step - loss: 5.9151 - acc: 0.0627\n",
      "Epoch 55/100\n",
      "9519/9519 [==============================] - 9s 914us/step - loss: 5.7887 - acc: 0.0633\n",
      "Epoch 56/100\n",
      "9519/9519 [==============================] - 8s 869us/step - loss: 5.6741 - acc: 0.0685\n",
      "Epoch 57/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 5.5544 - acc: 0.0691\n",
      "Epoch 58/100\n",
      "9519/9519 [==============================] - 12s 1ms/step - loss: 5.4346 - acc: 0.0708\n",
      "Epoch 59/100\n",
      "9519/9519 [==============================] - 14s 1ms/step - loss: 5.3210 - acc: 0.0721\n",
      "Epoch 60/100\n",
      "9519/9519 [==============================] - 11s 1ms/step - loss: 5.2102 - acc: 0.0752\n",
      "Epoch 61/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 5.1060 - acc: 0.0801\n",
      "Epoch 62/100\n",
      "9519/9519 [==============================] - 11s 1ms/step - loss: 5.0008 - acc: 0.0819\n",
      "Epoch 63/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 4.8951 - acc: 0.0891\n",
      "Epoch 64/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 4.7913 - acc: 0.0969\n",
      "Epoch 65/100\n",
      "9519/9519 [==============================] - 13s 1ms/step - loss: 4.6916 - acc: 0.1028\n",
      "Epoch 66/100\n",
      "9519/9519 [==============================] - 12s 1ms/step - loss: 4.5956 - acc: 0.1132\n",
      "Epoch 67/100\n",
      "9519/9519 [==============================] - 13s 1ms/step - loss: 4.4962 - acc: 0.1227\n",
      "Epoch 68/100\n",
      "9519/9519 [==============================] - 14s 2ms/step - loss: 4.4034 - acc: 0.1315\n",
      "Epoch 69/100\n",
      "9519/9519 [==============================] - 12s 1ms/step - loss: 4.3103 - acc: 0.1433\n",
      "Epoch 70/100\n",
      "9519/9519 [==============================] - 12s 1ms/step - loss: 4.2210 - acc: 0.1521\n",
      "Epoch 71/100\n",
      "9519/9519 [==============================] - 13s 1ms/step - loss: 4.1307 - acc: 0.1668\n",
      "Epoch 72/100\n",
      "9519/9519 [==============================] - 11s 1ms/step - loss: 4.0467 - acc: 0.1759\n",
      "Epoch 73/100\n",
      "9519/9519 [==============================] - 13s 1ms/step - loss: 3.9582 - acc: 0.1922\n",
      "Epoch 74/100\n",
      "9519/9519 [==============================] - 12s 1ms/step - loss: 3.8833 - acc: 0.2040\n",
      "Epoch 75/100\n",
      "9519/9519 [==============================] - 11s 1ms/step - loss: 3.8020 - acc: 0.2190\n",
      "Epoch 76/100\n",
      "9519/9519 [==============================] - 11s 1ms/step - loss: 3.7305 - acc: 0.2298\n",
      "Epoch 77/100\n",
      "9519/9519 [==============================] - 16s 2ms/step - loss: 3.6569 - acc: 0.2407\n",
      "Epoch 78/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 3.5902 - acc: 0.2535\n",
      "Epoch 79/100\n",
      "9519/9519 [==============================] - 12s 1ms/step - loss: 3.5224 - acc: 0.2654\n",
      "Epoch 80/100\n",
      "9519/9519 [==============================] - 14s 2ms/step - loss: 3.4594 - acc: 0.2772\n",
      "Epoch 81/100\n",
      "9519/9519 [==============================] - 13s 1ms/step - loss: 3.3983 - acc: 0.2875\n",
      "Epoch 82/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 3.3392 - acc: 0.2938\n",
      "Epoch 83/100\n",
      "9519/9519 [==============================] - 11s 1ms/step - loss: 3.2786 - acc: 0.3048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100\n",
      "9519/9519 [==============================] - 13s 1ms/step - loss: 3.2261 - acc: 0.3128\n",
      "Epoch 85/100\n",
      "9519/9519 [==============================] - 12s 1ms/step - loss: 3.1688 - acc: 0.3238\n",
      "Epoch 86/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 3.1182 - acc: 0.3362\n",
      "Epoch 87/100\n",
      "9519/9519 [==============================] - 13s 1ms/step - loss: 3.0695 - acc: 0.3411\n",
      "Epoch 88/100\n",
      "9519/9519 [==============================] - 12s 1ms/step - loss: 3.0218 - acc: 0.3529\n",
      "Epoch 89/100\n",
      "9519/9519 [==============================] - 12s 1ms/step - loss: 2.9748 - acc: 0.3603\n",
      "Epoch 90/100\n",
      "9519/9519 [==============================] - 11s 1ms/step - loss: 2.9305 - acc: 0.3668\n",
      "Epoch 91/100\n",
      "9519/9519 [==============================] - 11s 1ms/step - loss: 2.8820 - acc: 0.3761\n",
      "Epoch 92/100\n",
      "9519/9519 [==============================] - 11s 1ms/step - loss: 2.8396 - acc: 0.3848\n",
      "Epoch 93/100\n",
      "9519/9519 [==============================] - 11s 1ms/step - loss: 2.7961 - acc: 0.3917\n",
      "Epoch 94/100\n",
      "9519/9519 [==============================] - 11s 1ms/step - loss: 2.7580 - acc: 0.4010\n",
      "Epoch 95/100\n",
      "9519/9519 [==============================] - 11s 1ms/step - loss: 2.7208 - acc: 0.4072\n",
      "Epoch 96/100\n",
      "9519/9519 [==============================] - 10s 1ms/step - loss: 2.6800 - acc: 0.4152\n",
      "Epoch 97/100\n",
      "9519/9519 [==============================] - 11s 1ms/step - loss: 2.6384 - acc: 0.4230\n",
      "Epoch 98/100\n",
      "9519/9519 [==============================] - 12s 1ms/step - loss: 2.6067 - acc: 0.4328\n",
      "Epoch 99/100\n",
      "9519/9519 [==============================] - 11s 1ms/step - loss: 2.5676 - acc: 0.4375\n",
      "Epoch 100/100\n",
      "9519/9519 [==============================] - 12s 1ms/step - loss: 2.5268 - acc: 0.4468\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 11, 10)            22730     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 11, 150)           96600     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               100400    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2273)              229573    \n",
      "=================================================================\n",
      "Total params: 449,303\n",
      "Trainable params: 449,303\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "frosty_model = create_model(frosty_predictors,\n",
    "                            frosty_label, \n",
    "                            frosty_max_sequence_len, \n",
    "                            frosty_total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 50976/307631 [===>..........................] - ETA: 3:03:34 - loss: 7.2434 - acc: 0.0292"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9168a4090f66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                           \u001b[0mslim_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                           \u001b[0mslim_max_sequence_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                           slim_total_words)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-b1e4ce8af157>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(predictors, label, max_sequence_len, total_words)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mearlystop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearlystop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "slim_model = create_model(slim_predictors, \n",
    "                          slim_label, \n",
    "                          slim_max_sequence_len, \n",
    "                          slim_total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_text, next_words, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "# List of all the state variables in the system and their initial values\n",
    "\n",
    "prompt_string = \"Double down divergent road rap\"\n",
    "\n",
    "initial_conditions = {\n",
    "    'transcript': prompt_string, # collaborative text\n",
    "    'slim_says' : \"\",\n",
    "    'frost_says': \"\",\n",
    "    'slim': (0.0,0.0, 0.0), #slim's opinions: scores of the suggestions, (fullprior,+slim,+frosty) \n",
    "    'frosty':(0.0,0.0, 0.0) #frosty opinions: scores of the suggestions, (fullprior,+slim,+frosty) \n",
    "}\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extra utility, may not be needed\n",
    "def new_line(transcript, max_sequence_len=12):\n",
    "    \n",
    "    current_line = transcript.split('\\n')[-1]\n",
    "    len_current_line = current_line.split(' ')\n",
    "    \n",
    "    val = bool(len_current_line>max_sequence_len)\n",
    "    \n",
    "    return val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slim_suggestions(params, step, sL, s):\n",
    "    \n",
    "    #user model evaluate\n",
    "    transcript = s['transcript']\n",
    "    current_line = transcript.split('\\n')[-1]\n",
    "    \n",
    "    lam = 3\n",
    "    rv = sts.poisson.rvs(lam)\n",
    "    \n",
    "    string = generate_text(slim_model, current_line, rv, frosty_max_sequence_len)\n",
    "    \n",
    "    #higher should be better\n",
    "    return {'slim_says': string}\n",
    "\n",
    "def get_frosty_suggestion(params, step, sL, s):\n",
    "    \n",
    "    #user model Generate:\n",
    "    transcript = s['transcript']\n",
    "    current_line = transcript.split('\\n')[-1]\n",
    "    \n",
    "    lam = 3\n",
    "    rv = sts.poisson.rvs(lam)\n",
    "    \n",
    "    string = generate_text(frosty_model,current_line, rv, frosty_max_sequence_len)\n",
    "    \n",
    "    #higher should be better\n",
    "    return {'frosty_says': string}\n",
    "\n",
    "def store_slim_suggestion(params, step, sL, s, _input):\n",
    "    \n",
    "    y = 'slim_says'\n",
    "    x = _input['slim_says']\n",
    "    \n",
    "    return(y, x)\n",
    "\n",
    "def store_frosty_suggestion(params, step, sL, s, _input):\n",
    "    \n",
    "    y = 'frosty_says'\n",
    "    x = _input['frosty_says']\n",
    "    \n",
    "    return(y, x) \n",
    "\n",
    "# IMPROVE - Tune\n",
    "def collaborate(params, step, sL, s):\n",
    "    \n",
    "    #use the suggestions and compute opinions\n",
    "    slim_says = s['slim_says']\n",
    "    frosty_says = s['frost_says']\n",
    "    prior_transcript = s['transcript']    \n",
    "    \n",
    "    #some function of evaluate methods from the RNNS\n",
    "    \n",
    "    #evaluates the existing script\n",
    "    slim_opinion_prior_transcript = get_slim_opinion(prior_transcript, \"\")\n",
    "    frosty_opinion_prior_transcript = get_frosty_opinion(prior_transcript, \"\")\n",
    "    \n",
    "    #evaluates the scipts with the new words\n",
    "    frosty_opinion_frosty_says =get_frosty_opinion(prior_transcript, frosty_says)\n",
    "    slim_opinion_frosty_says = get_slim_opinion(prior_transcript, frosty_says)\n",
    "    slim_opinion_slim_says = get_slim_opinion(prior_transcript, slim_says)\n",
    "    frosty_opinion_slim_says = get_frosty_opinion(prior_transcript, slim_says)\n",
    "    \n",
    "    opinion_dict = {\n",
    "                    'slim':(slim_opinion_prior_transcript,\n",
    "                            lim_opinion_slim_says,\n",
    "                            slim_opinion_frosty_says), \n",
    "                    'frosty':(frosty_opinion_prior_transcript,\n",
    "                             frosty_opinion_slim_says,\n",
    "                             frosty_opinion_frosty_says)\n",
    "                   }\n",
    "    \n",
    "    return opinion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE - call evaluate function\n",
    "def get_slim_opinion(transcript, suggested_word):\n",
    "\n",
    "    #use slim_model evaluate\n",
    "    \n",
    "    #higher should be better\n",
    "    return score\n",
    "\n",
    "def get_frosty_opinion(transcript, suggested_word):\n",
    "    \n",
    "    #use frosty_model evaluate\n",
    "    \n",
    "    #higher should be better\n",
    "    return score\n",
    "\n",
    "def choose(transcript, suggestions, opinions):\n",
    "    \n",
    "    #suggestions (slim, frosty) and opinions (slim, frosty)\n",
    "    \n",
    "    #heurstic funciton of the opinions returns 0 or 1\n",
    "    choice = 0 #or 1\n",
    "\n",
    "    return suggestions[choice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_transcript(params, step, sL, s, _input):\n",
    "    \n",
    "    prior_transcript = s['transcript']\n",
    "    opinions = (_input['slim'],  _input['frosty'])\n",
    "    suggestions = (s['slim_says'], s['frosty_says'])\n",
    "    \n",
    "    y = 'transcript'\n",
    "    \n",
    "    x = choose(prior_transcript, suggestions, opinions)\n",
    "    \n",
    "    return (y, x)\n",
    "\n",
    "def store_slim_opinion(params, step, sL, s, _input):\n",
    "    \n",
    "    opinion = _input['slim']\n",
    "    \n",
    "    y = 'slim'\n",
    "    \n",
    "    x = opinion\n",
    "    \n",
    "    return (y, x)\n",
    "\n",
    "def store_frosty_opinion(params, step, sL, s, _input):\n",
    "    \n",
    "    opinion = _input['frosty']\n",
    "    \n",
    "    y = 'frosty'\n",
    "    \n",
    "    x = opinion\n",
    "    \n",
    "    return (y, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "# In the Partial State Update Blocks, the user specifies if state update functions will be run in series or in parallel\n",
    "partial_state_update_blocks = [\n",
    "    { \n",
    "        'policies': {\n",
    "            'slim':get_slim_suggestion,\n",
    "            'frosty':get_frosty_suggestion \n",
    "        },\n",
    "        'variables': { # The following state variables will be updated simultaneously\n",
    "            'slim_says': store_slim_suggestion,\n",
    "            'frosty_says': store_frosty_suggestion\n",
    "        }\n",
    "    },\n",
    "    { \n",
    "        'policies': {\n",
    "            'collaborate': collaborate # Improve\n",
    "        },\n",
    "        'variables': { # The following state variables will be updated simultaneously\n",
    "            'transcript': update_transcript,\n",
    "            'slim':store_slim_opinions,\n",
    "            'frosty': store_frosty_opinions\n",
    "        }\n",
    "    }\n",
    "]\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Configuration Parameters\n",
    "Lastly, we define the number of timesteps and the number of Monte Carlo runs of the simulation. These parameters must be passed in a dictionary, in `dict_keys` `T` and `N`, respectively. In our example, we'll run the simulation for 10 timesteps. And because we are dealing with a deterministic system, it makes no sense to have multiple Monte Carlo runs, so we set `N=1`. We'll ignore the `M` key for now and set it to an empty `dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "# Settings of general simulation parameters, unrelated to the system itself\n",
    "# `T` is a range with the number of discrete units of time the simulation will run for;\n",
    "# `N` is the number of times the simulation will be run (Monte Carlo runs)\n",
    "# In this example, we'll run the simulation once (N=1) and its duration will be of 10 timesteps\n",
    "# We'll cover the `M` key in a future article. For now, let's leave it empty\n",
    "simulation_parameters = {\n",
    "    'T': range(10),\n",
    "    'N': 1,\n",
    "    'M': {}\n",
    "}\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "We have defined the state variables of our system and their initial conditions, as well as the state update functions, which have been grouped in a single state update block. We have also specified the parameters of the simulation (number of timesteps and runs). We are now ready to put all those pieces together in a `Configuration` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cadCAD.configuration import Configuration\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "# The configurations above are then packaged into a `Configuration` object\n",
    "config = Configuration(initial_state=initial_conditions, #dict containing variable names and initial values\n",
    "                       partial_state_update_blocks=partial_state_update_blocks, #dict containing state update functions\n",
    "                       sim_config=simulation_parameters #dict containing simulation parameters\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the engine\n",
    "We are now ready to run the engine with the configuration defined above. Instantiate an ExecutionMode, an ExecutionContext and an Executor objects, passing the Configuration object to the latter. Then run the `main()` method of the Executor object, which returns the results of the experiment in the first element of a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from cadCAD.engine import ExecutionMode, ExecutionContext, Executor\n",
    "exec_mode = ExecutionMode()\n",
    "exec_context = ExecutionContext(exec_mode.single_proc)\n",
    "executor = Executor(exec_context, [config]) # Pass the configuration object inside an array\n",
    "raw_result, tensor = executor.main() # The `main()` method returns a tuple; its first elements contains the raw results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the results\n",
    "We can now convert the raw results into a DataFrame for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(raw_result)\n",
    "df.set_index(['run', 'timestep', 'substep'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_About BlockScience_  \n",
    "[BlockScience](http://bit.ly/github_articles_M_1) is a research and engineering firm specialized in complex adaptive systems and applying practical methodologies from engineering design, development and testing to projects in emerging technologies such as blockchain. Follow us on [Medium](http://bit.ly/bsci-medium) or [Twitter](http://bit.ly/bsci-twitter) to stay in touch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
