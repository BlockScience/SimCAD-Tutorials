{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "import numpy as np \n",
    "\n",
    "import scipy.stats as sts\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "frosty_tokenizer = Tokenizer()\n",
    "frosty_data = open('robert_frost.txt').read()\n",
    "\n",
    "slim_tokenizer = Tokenizer()\n",
    "slim_data = open('slim_shady.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_preparation(data, tokenizer):\n",
    "\n",
    "    # basic cleanup\n",
    "    corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "    # tokenization\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "    # create input sequences using list of tokens\n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "\n",
    "    # pad sequences \n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "    # create predictors and label\n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "\n",
    "    return predictors, label, max_sequence_len, total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim_predictors, slim_label, slim_max_sequence_len, slim_total_words = dataset_preparation(slim_data,slim_tokenizer)\n",
    "frosty_predictors, frosty_label, frosty_max_sequence_len, frosty_total_words = dataset_preparation(frosty_data,frosty_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(predictors, label, max_sequence_len, total_words):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))\n",
    "    model.add(LSTM(150, return_sequences = True))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "    model.fit(predictors, label, epochs=100, verbose=1, callbacks=[earlystop])\n",
    "    print(model.summary())\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9519/9519 [==============================] - 6s 616us/step - loss: 6.9028 - acc: 0.0475\n",
      "Epoch 2/100\n",
      " 320/9519 [>.............................] - ETA: 5s - loss: 6.2977 - acc: 0.0344   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:526: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9519/9519 [==============================] - 5s 535us/step - loss: 6.5063 - acc: 0.0486\n",
      "Epoch 3/100\n",
      "9519/9519 [==============================] - 5s 536us/step - loss: 6.4783 - acc: 0.0482\n",
      "Epoch 4/100\n",
      "9519/9519 [==============================] - 5s 538us/step - loss: 6.4705 - acc: 0.0479\n",
      "Epoch 5/100\n",
      "9519/9519 [==============================] - 5s 539us/step - loss: 6.4681 - acc: 0.0486\n",
      "Epoch 6/100\n",
      "9519/9519 [==============================] - 5s 540us/step - loss: 6.4704 - acc: 0.0486\n",
      "Epoch 7/100\n",
      "9519/9519 [==============================] - 5s 541us/step - loss: 6.4686 - acc: 0.0486\n",
      "Epoch 8/100\n",
      "9519/9519 [==============================] - 5s 547us/step - loss: 6.4665 - acc: 0.0480\n",
      "Epoch 9/100\n",
      "9519/9519 [==============================] - 5s 546us/step - loss: 6.4672 - acc: 0.0486\n",
      "Epoch 10/100\n",
      "9519/9519 [==============================] - 5s 549us/step - loss: 6.4685 - acc: 0.0486\n",
      "Epoch 11/100\n",
      "9519/9519 [==============================] - 5s 548us/step - loss: 6.4645 - acc: 0.0486\n",
      "Epoch 12/100\n",
      "9519/9519 [==============================] - 5s 547us/step - loss: 6.4571 - acc: 0.0478\n",
      "Epoch 13/100\n",
      "9519/9519 [==============================] - 5s 549us/step - loss: 6.4520 - acc: 0.0479\n",
      "Epoch 14/100\n",
      "9519/9519 [==============================] - 5s 549us/step - loss: 6.4436 - acc: 0.0486\n",
      "Epoch 15/100\n",
      "9519/9519 [==============================] - 5s 551us/step - loss: 6.4399 - acc: 0.0486\n",
      "Epoch 16/100\n",
      "9519/9519 [==============================] - 5s 550us/step - loss: 6.4351 - acc: 0.0486\n",
      "Epoch 17/100\n",
      "9519/9519 [==============================] - 5s 548us/step - loss: 6.4330 - acc: 0.0486\n",
      "Epoch 18/100\n",
      "9519/9519 [==============================] - 5s 552us/step - loss: 6.4298 - acc: 0.0470\n",
      "Epoch 19/100\n",
      "9519/9519 [==============================] - 5s 552us/step - loss: 6.4259 - acc: 0.0486\n",
      "Epoch 20/100\n",
      "9519/9519 [==============================] - 5s 556us/step - loss: 6.4228 - acc: 0.0486\n",
      "Epoch 21/100\n",
      "9519/9519 [==============================] - 5s 555us/step - loss: 6.4163 - acc: 0.0481\n",
      "Epoch 22/100\n",
      "9519/9519 [==============================] - 5s 560us/step - loss: 6.4139 - acc: 0.0486\n",
      "Epoch 23/100\n",
      "9519/9519 [==============================] - 5s 555us/step - loss: 6.4125 - acc: 0.0487\n",
      "Epoch 24/100\n",
      "9519/9519 [==============================] - 5s 555us/step - loss: 6.4120 - acc: 0.0490\n",
      "Epoch 25/100\n",
      "9519/9519 [==============================] - 5s 558us/step - loss: 6.4095 - acc: 0.0493\n",
      "Epoch 26/100\n",
      "9519/9519 [==============================] - 5s 555us/step - loss: 6.4091 - acc: 0.0492\n",
      "Epoch 27/100\n",
      "9519/9519 [==============================] - 5s 557us/step - loss: 6.4102 - acc: 0.0494\n",
      "Epoch 28/100\n",
      "9519/9519 [==============================] - 5s 558us/step - loss: 6.4095 - acc: 0.0490\n",
      "Epoch 29/100\n",
      "9519/9519 [==============================] - 5s 558us/step - loss: 6.4094 - acc: 0.0506\n",
      "Epoch 30/100\n",
      "9519/9519 [==============================] - 5s 557us/step - loss: 6.4089 - acc: 0.0502\n",
      "Epoch 31/100\n",
      "9519/9519 [==============================] - 5s 556us/step - loss: 6.4158 - acc: 0.0488\n",
      "Epoch 32/100\n",
      "9519/9519 [==============================] - 5s 560us/step - loss: 6.4113 - acc: 0.0488\n",
      "Epoch 33/100\n",
      "9519/9519 [==============================] - 5s 560us/step - loss: 6.4186 - acc: 0.0511\n",
      "Epoch 34/100\n",
      "9519/9519 [==============================] - 5s 560us/step - loss: 6.3822 - acc: 0.0536\n",
      "Epoch 35/100\n",
      "9519/9519 [==============================] - 5s 561us/step - loss: 6.3295 - acc: 0.0535\n",
      "Epoch 36/100\n",
      "9519/9519 [==============================] - 5s 563us/step - loss: 6.1850 - acc: 0.0559\n",
      "Epoch 37/100\n",
      "9519/9519 [==============================] - 5s 559us/step - loss: 6.0308 - acc: 0.0568\n",
      "Epoch 38/100\n",
      "9519/9519 [==============================] - 5s 560us/step - loss: 5.8995 - acc: 0.0589\n",
      "Epoch 39/100\n",
      "9519/9519 [==============================] - 5s 562us/step - loss: 5.7863 - acc: 0.0594\n",
      "Epoch 40/100\n",
      "9519/9519 [==============================] - 5s 561us/step - loss: 5.6803 - acc: 0.0628\n",
      "Epoch 41/100\n",
      "9519/9519 [==============================] - 5s 565us/step - loss: 5.5784 - acc: 0.0651\n",
      "Epoch 42/100\n",
      "9519/9519 [==============================] - 5s 562us/step - loss: 5.4886 - acc: 0.0670\n",
      "Epoch 43/100\n",
      "9519/9519 [==============================] - 5s 563us/step - loss: 5.4079 - acc: 0.0691\n",
      "Epoch 44/100\n",
      "9519/9519 [==============================] - 5s 566us/step - loss: 5.3283 - acc: 0.0719\n",
      "Epoch 45/100\n",
      "9519/9519 [==============================] - 5s 566us/step - loss: 5.2517 - acc: 0.0733\n",
      "Epoch 46/100\n",
      "9519/9519 [==============================] - 5s 564us/step - loss: 5.1779 - acc: 0.0778\n",
      "Epoch 47/100\n",
      "9519/9519 [==============================] - 5s 563us/step - loss: 5.1071 - acc: 0.0785\n",
      "Epoch 48/100\n",
      "9519/9519 [==============================] - 5s 564us/step - loss: 5.0353 - acc: 0.0817\n",
      "Epoch 49/100\n",
      "9519/9519 [==============================] - 5s 565us/step - loss: 4.9645 - acc: 0.0854\n",
      "Epoch 50/100\n",
      "9519/9519 [==============================] - 5s 572us/step - loss: 4.8945 - acc: 0.0875\n",
      "Epoch 51/100\n",
      "9519/9519 [==============================] - 5s 571us/step - loss: 4.8246 - acc: 0.0894\n",
      "Epoch 52/100\n",
      "9519/9519 [==============================] - 5s 571us/step - loss: 4.7595 - acc: 0.0896\n",
      "Epoch 53/100\n",
      "9519/9519 [==============================] - 5s 573us/step - loss: 4.6912 - acc: 0.0955\n",
      "Epoch 54/100\n",
      "9519/9519 [==============================] - 6s 583us/step - loss: 4.6310 - acc: 0.0962\n",
      "Epoch 55/100\n",
      "9519/9519 [==============================] - 6s 585us/step - loss: 4.5709 - acc: 0.1038\n",
      "Epoch 56/100\n",
      "9519/9519 [==============================] - 5s 570us/step - loss: 4.5080 - acc: 0.1110\n",
      "Epoch 57/100\n",
      "9519/9519 [==============================] - 6s 631us/step - loss: 4.4508 - acc: 0.1151\n",
      "Epoch 58/100\n",
      "9519/9519 [==============================] - 6s 636us/step - loss: 4.3945 - acc: 0.1214\n",
      "Epoch 59/100\n",
      "9519/9519 [==============================] - 5s 522us/step - loss: 4.3364 - acc: 0.1293\n",
      "Epoch 60/100\n",
      "9519/9519 [==============================] - 5s 542us/step - loss: 4.2816 - acc: 0.1348\n",
      "Epoch 61/100\n",
      "9519/9519 [==============================] - 6s 596us/step - loss: 4.2273 - acc: 0.1408\n",
      "Epoch 62/100\n",
      "9519/9519 [==============================] - 5s 568us/step - loss: 4.1777 - acc: 0.1479\n",
      "Epoch 63/100\n",
      "9519/9519 [==============================] - 5s 511us/step - loss: 4.1250 - acc: 0.1569\n",
      "Epoch 64/100\n",
      "9519/9519 [==============================] - 5s 521us/step - loss: 4.0775 - acc: 0.1604\n",
      "Epoch 65/100\n",
      "9519/9519 [==============================] - 5s 528us/step - loss: 4.0297 - acc: 0.1698\n",
      "Epoch 66/100\n",
      "9519/9519 [==============================] - 5s 513us/step - loss: 3.9816 - acc: 0.1781\n",
      "Epoch 67/100\n",
      "9519/9519 [==============================] - 5s 513us/step - loss: 3.9341 - acc: 0.1841\n",
      "Epoch 68/100\n",
      "9519/9519 [==============================] - 5s 508us/step - loss: 3.8925 - acc: 0.1906\n",
      "Epoch 69/100\n",
      "9519/9519 [==============================] - 5s 511us/step - loss: 3.8491 - acc: 0.1949\n",
      "Epoch 70/100\n",
      "9519/9519 [==============================] - 5s 513us/step - loss: 3.8046 - acc: 0.2037\n",
      "Epoch 71/100\n",
      "9519/9519 [==============================] - 5s 511us/step - loss: 3.7627 - acc: 0.2132\n",
      "Epoch 72/100\n",
      "9519/9519 [==============================] - 5s 514us/step - loss: 3.7241 - acc: 0.2186\n",
      "Epoch 73/100\n",
      "9519/9519 [==============================] - 5s 515us/step - loss: 3.6883 - acc: 0.2244\n",
      "Epoch 74/100\n",
      "9519/9519 [==============================] - 5s 567us/step - loss: 3.6493 - acc: 0.2309\n",
      "Epoch 75/100\n",
      "9519/9519 [==============================] - 5s 562us/step - loss: 3.6111 - acc: 0.2345\n",
      "Epoch 76/100\n",
      "9519/9519 [==============================] - 6s 593us/step - loss: 3.5735 - acc: 0.2449\n",
      "Epoch 77/100\n",
      "9519/9519 [==============================] - 5s 527us/step - loss: 3.5395 - acc: 0.2512\n",
      "Epoch 78/100\n",
      "9519/9519 [==============================] - 5s 493us/step - loss: 3.5025 - acc: 0.2543\n",
      "Epoch 79/100\n",
      "9519/9519 [==============================] - 5s 531us/step - loss: 3.4732 - acc: 0.2589\n",
      "Epoch 80/100\n",
      "9519/9519 [==============================] - 5s 519us/step - loss: 3.4417 - acc: 0.2624\n",
      "Epoch 81/100\n",
      "9519/9519 [==============================] - 5s 514us/step - loss: 3.4087 - acc: 0.2694\n",
      "Epoch 82/100\n",
      "9519/9519 [==============================] - 5s 529us/step - loss: 3.3759 - acc: 0.2761\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9519/9519 [==============================] - 5s 510us/step - loss: 3.3430 - acc: 0.2830\n",
      "Epoch 84/100\n",
      "9519/9519 [==============================] - 5s 509us/step - loss: 3.3150 - acc: 0.2850\n",
      "Epoch 85/100\n",
      "9519/9519 [==============================] - 5s 490us/step - loss: 3.2832 - acc: 0.2926\n",
      "Epoch 86/100\n",
      "9519/9519 [==============================] - 5s 490us/step - loss: 3.2558 - acc: 0.2965\n",
      "Epoch 87/100\n",
      "9519/9519 [==============================] - 5s 489us/step - loss: 3.2232 - acc: 0.3009\n",
      "Epoch 88/100\n",
      "9519/9519 [==============================] - 5s 490us/step - loss: 3.1971 - acc: 0.3078\n",
      "Epoch 89/100\n",
      "9519/9519 [==============================] - 5s 490us/step - loss: 3.1651 - acc: 0.3162\n",
      "Epoch 90/100\n",
      "9519/9519 [==============================] - 5s 490us/step - loss: 3.1407 - acc: 0.3178\n",
      "Epoch 91/100\n",
      "9519/9519 [==============================] - 5s 530us/step - loss: 3.1183 - acc: 0.3241\n",
      "Epoch 92/100\n",
      "9519/9519 [==============================] - 6s 636us/step - loss: 3.0911 - acc: 0.3268\n",
      "Epoch 93/100\n",
      "9519/9519 [==============================] - 5s 492us/step - loss: 3.0643 - acc: 0.3357\n",
      "Epoch 94/100\n",
      "9519/9519 [==============================] - 5s 569us/step - loss: 3.0375 - acc: 0.3375\n",
      "Epoch 95/100\n",
      "9519/9519 [==============================] - 6s 585us/step - loss: 3.0116 - acc: 0.3430\n",
      "Epoch 96/100\n",
      "9519/9519 [==============================] - 6s 580us/step - loss: 2.9946 - acc: 0.3454\n",
      "Epoch 97/100\n",
      "9519/9519 [==============================] - 6s 582us/step - loss: 2.9633 - acc: 0.3504\n",
      "Epoch 98/100\n",
      "9519/9519 [==============================] - 6s 581us/step - loss: 2.9460 - acc: 0.3553\n",
      "Epoch 99/100\n",
      "9519/9519 [==============================] - 6s 583us/step - loss: 2.9193 - acc: 0.3593\n",
      "Epoch 100/100\n",
      "9519/9519 [==============================] - 6s 584us/step - loss: 2.8992 - acc: 0.3647\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 11, 10)            22730     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 11, 150)           96600     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               100400    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2273)              229573    \n",
      "=================================================================\n",
      "Total params: 449,303\n",
      "Trainable params: 449,303\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "frosty_model = create_model(frosty_predictors,\n",
    "                            frosty_label, \n",
    "                            frosty_max_sequence_len, \n",
    "                            frosty_total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "307631/307631 [==============================] - 5604s 18ms/step - loss: 7.1815 - acc: 0.0305\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:526: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307631/307631 [==============================] - 5710s 19ms/step - loss: 7.1414 - acc: 0.0303\n",
      "Epoch 3/100\n",
      "307631/307631 [==============================] - 5795s 19ms/step - loss: 7.1428 - acc: 0.0308\n",
      "Epoch 4/100\n",
      "307631/307631 [==============================] - 5803s 19ms/step - loss: 7.1451 - acc: 0.0305\n",
      "Epoch 5/100\n",
      "307631/307631 [==============================] - 5861s 19ms/step - loss: 7.1465 - acc: 0.0307\n",
      "Epoch 6/100\n",
      "307631/307631 [==============================] - 5803s 19ms/step - loss: 7.1495 - acc: 0.0305\n",
      "Epoch 7/100\n",
      "307631/307631 [==============================] - 5765s 19ms/step - loss: 7.1513 - acc: 0.0308\n",
      "Epoch 8/100\n",
      "307631/307631 [==============================] - 5774s 19ms/step - loss: 7.1529 - acc: 0.0307\n",
      "Epoch 9/100\n",
      "307631/307631 [==============================] - 5753s 19ms/step - loss: 7.1528 - acc: 0.0305\n",
      "Epoch 10/100\n",
      "307631/307631 [==============================] - 5727s 19ms/step - loss: 7.1547 - acc: 0.0307\n",
      "Epoch 11/100\n",
      "307631/307631 [==============================] - 5699s 19ms/step - loss: 7.1546 - acc: 0.0300\n",
      "Epoch 12/100\n",
      "307631/307631 [==============================] - 5673s 18ms/step - loss: 7.1558 - acc: 0.0305\n",
      "Epoch 13/100\n",
      "307631/307631 [==============================] - 5665s 18ms/step - loss: 7.1573 - acc: 0.0306\n",
      "Epoch 14/100\n",
      "307631/307631 [==============================] - 5650s 18ms/step - loss: 7.1576 - acc: 0.0311\n",
      "Epoch 15/100\n",
      "307631/307631 [==============================] - 5645s 18ms/step - loss: 7.1588 - acc: 0.0307\n",
      "Epoch 16/100\n",
      "307631/307631 [==============================] - 5613s 18ms/step - loss: 7.1590 - acc: 0.0310\n",
      "Epoch 17/100\n",
      "307631/307631 [==============================] - 5689s 18ms/step - loss: 7.1589 - acc: 0.0309\n",
      "Epoch 18/100\n",
      "307631/307631 [==============================] - 5721s 19ms/step - loss: 7.1599 - acc: 0.0300\n",
      "Epoch 19/100\n",
      "307631/307631 [==============================] - 5701s 19ms/step - loss: 7.1613 - acc: 0.0303\n",
      "Epoch 20/100\n",
      "307631/307631 [==============================] - 5656s 18ms/step - loss: 7.1604 - acc: 0.0306\n",
      "Epoch 21/100\n",
      "307631/307631 [==============================] - 5652s 18ms/step - loss: 7.1622 - acc: 0.0310\n",
      "Epoch 22/100\n",
      "307631/307631 [==============================] - 5646s 18ms/step - loss: 7.1619 - acc: 0.0305\n",
      "Epoch 23/100\n",
      "307631/307631 [==============================] - 5663s 18ms/step - loss: 7.1639 - acc: 0.0305\n",
      "Epoch 24/100\n",
      "307631/307631 [==============================] - 5650s 18ms/step - loss: 7.1639 - acc: 0.0305\n",
      "Epoch 25/100\n",
      "307631/307631 [==============================] - 5637s 18ms/step - loss: 7.1644 - acc: 0.0306\n",
      "Epoch 26/100\n",
      "307631/307631 [==============================] - 5629s 18ms/step - loss: 7.1642 - acc: 0.0307\n",
      "Epoch 27/100\n",
      "307631/307631 [==============================] - 5613s 18ms/step - loss: 7.1652 - acc: 0.0304\n",
      "Epoch 28/100\n",
      "307631/307631 [==============================] - 5593s 18ms/step - loss: 7.1662 - acc: 0.0304\n",
      "Epoch 29/100\n",
      "307631/307631 [==============================] - 5573s 18ms/step - loss: 7.1660 - acc: 0.0302\n",
      "Epoch 30/100\n",
      "307631/307631 [==============================] - 5549s 18ms/step - loss: 7.1669 - acc: 0.0309\n",
      "Epoch 31/100\n",
      "307631/307631 [==============================] - 5585s 18ms/step - loss: 7.1669 - acc: 0.0311\n",
      "Epoch 32/100\n",
      "307631/307631 [==============================] - 5686s 18ms/step - loss: 7.1666 - acc: 0.0305\n",
      "Epoch 33/100\n",
      "307631/307631 [==============================] - 5723s 19ms/step - loss: 7.1669 - acc: 0.0305\n",
      "Epoch 34/100\n",
      "307631/307631 [==============================] - 5729s 19ms/step - loss: 7.1680 - acc: 0.0308\n",
      "Epoch 35/100\n",
      "307631/307631 [==============================] - 5719s 19ms/step - loss: 7.1679 - acc: 0.0307\n",
      "Epoch 36/100\n",
      "307631/307631 [==============================] - 5717s 19ms/step - loss: 7.1695 - acc: 0.0305\n",
      "Epoch 37/100\n",
      "307631/307631 [==============================] - 5724s 19ms/step - loss: 7.1674 - acc: 0.0308\n",
      "Epoch 38/100\n",
      "307631/307631 [==============================] - 5748s 19ms/step - loss: 7.1684 - acc: 0.0307\n",
      "Epoch 39/100\n",
      "307631/307631 [==============================] - 5753s 19ms/step - loss: 7.1701 - acc: 0.0306\n",
      "Epoch 40/100\n",
      "307631/307631 [==============================] - 5760s 19ms/step - loss: 7.1693 - acc: 0.0306\n",
      "Epoch 41/100\n",
      "307631/307631 [==============================] - 5796s 19ms/step - loss: 7.1701 - acc: 0.0305\n",
      "Epoch 42/100\n",
      "307631/307631 [==============================] - 5791s 19ms/step - loss: 7.1697 - acc: 0.0302\n",
      "Epoch 43/100\n",
      "307631/307631 [==============================] - 5776s 19ms/step - loss: 7.1702 - acc: 0.0308\n",
      "Epoch 44/100\n",
      "307631/307631 [==============================] - 5768s 19ms/step - loss: 7.1713 - acc: 0.0302\n",
      "Epoch 45/100\n",
      "307631/307631 [==============================] - 5742s 19ms/step - loss: 7.1717 - acc: 0.0304\n",
      "Epoch 46/100\n",
      "307631/307631 [==============================] - 5736s 19ms/step - loss: 7.1714 - acc: 0.0306\n",
      "Epoch 47/100\n",
      "307631/307631 [==============================] - 5751s 19ms/step - loss: 7.1729 - acc: 0.0310\n",
      "Epoch 48/100\n",
      "307631/307631 [==============================] - 5765s 19ms/step - loss: 7.1715 - acc: 0.0303\n",
      "Epoch 49/100\n",
      "307631/307631 [==============================] - 5782s 19ms/step - loss: 7.1728 - acc: 0.0304\n",
      "Epoch 50/100\n",
      "307631/307631 [==============================] - 5798s 19ms/step - loss: 7.1721 - acc: 0.0308\n",
      "Epoch 51/100\n",
      "307631/307631 [==============================] - 5797s 19ms/step - loss: 7.1718 - acc: 0.0307\n",
      "Epoch 52/100\n",
      "307631/307631 [==============================] - 5821s 19ms/step - loss: 7.1727 - acc: 0.0306\n",
      "Epoch 53/100\n",
      "307631/307631 [==============================] - 5824s 19ms/step - loss: 7.1733 - acc: 0.0300\n",
      "Epoch 54/100\n",
      "307631/307631 [==============================] - 5820s 19ms/step - loss: 7.1724 - acc: 0.0312\n",
      "Epoch 55/100\n",
      "307631/307631 [==============================] - 5818s 19ms/step - loss: 7.1732 - acc: 0.0308\n",
      "Epoch 56/100\n",
      "307631/307631 [==============================] - 5807s 19ms/step - loss: 7.1740 - acc: 0.0305\n",
      "Epoch 57/100\n",
      "307631/307631 [==============================] - 5795s 19ms/step - loss: 7.1734 - acc: 0.0310\n",
      "Epoch 58/100\n",
      "307631/307631 [==============================] - 5779s 19ms/step - loss: 7.1745 - acc: 0.0307\n",
      "Epoch 59/100\n",
      "307631/307631 [==============================] - 5768s 19ms/step - loss: 7.1751 - acc: 0.0308\n",
      "Epoch 60/100\n",
      "307631/307631 [==============================] - 5744s 19ms/step - loss: 7.1745 - acc: 0.0305\n",
      "Epoch 61/100\n",
      "307631/307631 [==============================] - 5767s 19ms/step - loss: 7.1750 - acc: 0.0307\n",
      "Epoch 62/100\n",
      "307631/307631 [==============================] - 5805s 19ms/step - loss: 7.1760 - acc: 0.0306\n",
      "Epoch 63/100\n",
      "307631/307631 [==============================] - 5841s 19ms/step - loss: 7.1756 - acc: 0.0313\n",
      "Epoch 64/100\n",
      "307631/307631 [==============================] - 5870s 19ms/step - loss: 7.1750 - acc: 0.0297\n",
      "Epoch 65/100\n",
      "307631/307631 [==============================] - 5824s 19ms/step - loss: 7.1764 - acc: 0.0306\n",
      "Epoch 66/100\n",
      "307631/307631 [==============================] - 5907s 19ms/step - loss: 7.1767 - acc: 0.0303\n",
      "Epoch 67/100\n",
      "307631/307631 [==============================] - 5985s 19ms/step - loss: 7.1759 - acc: 0.0305\n",
      "Epoch 68/100\n",
      "307631/307631 [==============================] - 5967s 19ms/step - loss: 7.1772 - acc: 0.0307\n",
      "Epoch 69/100\n",
      "307631/307631 [==============================] - 5808s 19ms/step - loss: 7.1776 - acc: 0.0307\n",
      "Epoch 70/100\n",
      "307631/307631 [==============================] - 5820s 19ms/step - loss: 7.1772 - acc: 0.0311\n",
      "Epoch 71/100\n",
      "307631/307631 [==============================] - 5806s 19ms/step - loss: 7.1771 - acc: 0.0308\n",
      "Epoch 72/100\n",
      "307631/307631 [==============================] - 5787s 19ms/step - loss: 7.1780 - acc: 0.0306\n",
      "Epoch 73/100\n",
      "307631/307631 [==============================] - 5773s 19ms/step - loss: 7.1780 - acc: 0.0308\n",
      "Epoch 74/100\n",
      "307631/307631 [==============================] - 5748s 19ms/step - loss: 7.1773 - acc: 0.0307\n",
      "Epoch 75/100\n",
      "307631/307631 [==============================] - 5703s 19ms/step - loss: 7.1782 - acc: 0.0306\n",
      "Epoch 76/100\n",
      "307631/307631 [==============================] - 5708s 19ms/step - loss: 7.1787 - acc: 0.0305\n",
      "Epoch 77/100\n",
      "307631/307631 [==============================] - 5769s 19ms/step - loss: 7.1780 - acc: 0.0309\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307631/307631 [==============================] - 5842s 19ms/step - loss: 7.1772 - acc: 0.0304\n",
      "Epoch 79/100\n",
      "307631/307631 [==============================] - 5898s 19ms/step - loss: 7.1783 - acc: 0.0306\n",
      "Epoch 80/100\n",
      "307631/307631 [==============================] - 5942s 19ms/step - loss: 7.1781 - acc: 0.0305\n",
      "Epoch 81/100\n",
      "307631/307631 [==============================] - 5949s 19ms/step - loss: 7.1785 - acc: 0.0308\n",
      "Epoch 82/100\n",
      "307631/307631 [==============================] - 5955s 19ms/step - loss: 7.1793 - acc: 0.0313\n",
      "Epoch 83/100\n",
      "307631/307631 [==============================] - 5961s 19ms/step - loss: 7.1781 - acc: 0.0311\n",
      "Epoch 84/100\n",
      "307631/307631 [==============================] - 5970s 19ms/step - loss: 7.1795 - acc: 0.0307\n",
      "Epoch 85/100\n",
      "307631/307631 [==============================] - 5969s 19ms/step - loss: 7.1780 - acc: 0.0309\n",
      "Epoch 86/100\n",
      "307631/307631 [==============================] - 5962s 19ms/step - loss: 7.1786 - acc: 0.0311\n",
      "Epoch 87/100\n",
      "307631/307631 [==============================] - 5952s 19ms/step - loss: 7.1802 - acc: 0.0302\n",
      "Epoch 88/100\n",
      "307631/307631 [==============================] - 5940s 19ms/step - loss: 7.1800 - acc: 0.0312\n",
      "Epoch 89/100\n",
      "307631/307631 [==============================] - 5921s 19ms/step - loss: 7.1805 - acc: 0.0305\n",
      "Epoch 90/100\n",
      "307631/307631 [==============================] - 5852s 19ms/step - loss: 7.1803 - acc: 0.0306\n",
      "Epoch 91/100\n",
      "307631/307631 [==============================] - 5860s 19ms/step - loss: 7.1800 - acc: 0.0305\n",
      "Epoch 92/100\n",
      "307631/307631 [==============================] - 5891s 19ms/step - loss: 7.1809 - acc: 0.0302\n",
      "Epoch 93/100\n",
      "307631/307631 [==============================] - 5909s 19ms/step - loss: 7.1806 - acc: 0.0305\n",
      "Epoch 94/100\n",
      "307631/307631 [==============================] - 5911s 19ms/step - loss: 7.1799 - acc: 0.0311\n",
      "Epoch 95/100\n",
      "307631/307631 [==============================] - 5906s 19ms/step - loss: 7.1808 - acc: 0.0311\n",
      "Epoch 96/100\n",
      "307631/307631 [==============================] - 5960s 19ms/step - loss: 7.1797 - acc: 0.0309\n",
      "Epoch 97/100\n",
      "307631/307631 [==============================] - 6002s 20ms/step - loss: 7.1813 - acc: 0.0306\n",
      "Epoch 98/100\n",
      "307631/307631 [==============================] - 6026s 20ms/step - loss: 7.1815 - acc: 0.0305\n",
      "Epoch 99/100\n",
      "307631/307631 [==============================] - 6036s 20ms/step - loss: 7.1816 - acc: 0.0304\n",
      "Epoch 100/100\n",
      "307631/307631 [==============================] - 6038s 20ms/step - loss: 7.1817 - acc: 0.0306\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 663, 10)           147790    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 663, 150)          96600     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               100400    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 14779)             1492679   \n",
      "=================================================================\n",
      "Total params: 1,837,469\n",
      "Trainable params: 1,837,469\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "slim_model = create_model(slim_predictors, \n",
    "                          slim_label, \n",
    "                          slim_max_sequence_len, \n",
    "                          slim_total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "slim_model_json = slim_model.to_json()\n",
    "with open(\"slim_model.json\", \"w\") as json_file:\n",
    "    json_file.write(slim_model_json)\n",
    "# serialize weights to HDF5\n",
    "slim_model.save_weights(\"slim_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('slim_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"slim_model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_text, next_words, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "# List of all the state variables in the system and their initial values\n",
    "\n",
    "prompt_string = \"Double down divergent road rap\"\n",
    "\n",
    "initial_conditions = {\n",
    "    'transcript': prompt_string, # collaborative text\n",
    "    'slim_says' : \"\",\n",
    "    'frost_says': \"\",\n",
    "    'slim': (0.0,0.0, 0.0), #slim's opinions: scores of the suggestions, (fullprior,+slim,+frosty) \n",
    "    'frosty':(0.0,0.0, 0.0) #frosty opinions: scores of the suggestions, (fullprior,+slim,+frosty) \n",
    "}\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extra utility, may not be needed\n",
    "def new_line(transcript, max_sequence_len=12):\n",
    "    \n",
    "    current_line = transcript.split('\\n')[-1]\n",
    "    len_current_line = current_line.split(' ')\n",
    "    \n",
    "    val = bool(len_current_line>max_sequence_len)\n",
    "    \n",
    "    return val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slim_suggestions(params, step, sL, s):\n",
    "    \n",
    "    #user model evaluate\n",
    "    transcript = s['transcript']\n",
    "    current_line = transcript.split('\\n')[-1]\n",
    "    \n",
    "    lam = 3\n",
    "    rv = sts.poisson.rvs(lam)\n",
    "    \n",
    "    string = generate_text(slim_model, current_line, rv, frosty_max_sequence_len)\n",
    "    \n",
    "    #higher should be better\n",
    "    return {'slim_says': string}\n",
    "\n",
    "def get_frosty_suggestion(params, step, sL, s):\n",
    "    \n",
    "    #user model Generate:\n",
    "    transcript = s['transcript']\n",
    "    current_line = transcript.split('\\n')[-1]\n",
    "    \n",
    "    lam = 3\n",
    "    rv = sts.poisson.rvs(lam)\n",
    "    \n",
    "    string = generate_text(frosty_model,current_line, rv, frosty_max_sequence_len)\n",
    "    \n",
    "    #higher should be better\n",
    "    return {'frosty_says': string}\n",
    "\n",
    "def store_slim_suggestion(params, step, sL, s, _input):\n",
    "    \n",
    "    y = 'slim_says'\n",
    "    x = _input['slim_says']\n",
    "    \n",
    "    return(y, x)\n",
    "\n",
    "def store_frosty_suggestion(params, step, sL, s, _input):\n",
    "    \n",
    "    y = 'frosty_says'\n",
    "    x = _input['frosty_says']\n",
    "    \n",
    "    return(y, x) \n",
    "\n",
    "# IMPROVE - Tune\n",
    "def collaborate(params, step, sL, s):\n",
    "    \n",
    "    #use the suggestions and compute opinions\n",
    "    slim_says = s['slim_says']\n",
    "    frosty_says = s['frost_says']\n",
    "    prior_transcript = s['transcript']    \n",
    "    \n",
    "    #some function of evaluate methods from the RNNS\n",
    "    \n",
    "    #evaluates the existing script\n",
    "    slim_opinion_prior_transcript = get_slim_opinion(prior_transcript, \"\")\n",
    "    frosty_opinion_prior_transcript = get_frosty_opinion(prior_transcript, \"\")\n",
    "    \n",
    "    #evaluates the scipts with the new words\n",
    "    frosty_opinion_frosty_says =get_frosty_opinion(prior_transcript, frosty_says)\n",
    "    slim_opinion_frosty_says = get_slim_opinion(prior_transcript, frosty_says)\n",
    "    slim_opinion_slim_says = get_slim_opinion(prior_transcript, slim_says)\n",
    "    frosty_opinion_slim_says = get_frosty_opinion(prior_transcript, slim_says)\n",
    "    \n",
    "    opinion_dict = {\n",
    "                    'slim':(slim_opinion_prior_transcript,\n",
    "                            lim_opinion_slim_says,\n",
    "                            slim_opinion_frosty_says), \n",
    "                    'frosty':(frosty_opinion_prior_transcript,\n",
    "                             frosty_opinion_slim_says,\n",
    "                             frosty_opinion_frosty_says)\n",
    "                   }\n",
    "    \n",
    "    return opinion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE - call evaluate function\n",
    "def get_slim_opinion(transcript, suggested_word):\n",
    "\n",
    "    #use slim_model evaluate\n",
    "    \n",
    "    #higher should be better\n",
    "    return score\n",
    "\n",
    "def get_frosty_opinion(transcript, suggested_word):\n",
    "    \n",
    "    #use frosty_model evaluate\n",
    "    \n",
    "    #higher should be better\n",
    "    return score\n",
    "\n",
    "def choose(transcript, suggestions, opinions):\n",
    "    \n",
    "    #suggestions (slim, frosty) and opinions (slim, frosty)\n",
    "    \n",
    "    #heurstic funciton of the opinions returns 0 or 1\n",
    "    choice = 0 #or 1\n",
    "\n",
    "    return suggestions[choice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_transcript(params, step, sL, s, _input):\n",
    "    \n",
    "    prior_transcript = s['transcript']\n",
    "    opinions = (_input['slim'],  _input['frosty'])\n",
    "    suggestions = (s['slim_says'], s['frosty_says'])\n",
    "    \n",
    "    y = 'transcript'\n",
    "    \n",
    "    x = choose(prior_transcript, suggestions, opinions)\n",
    "    \n",
    "    return (y, x)\n",
    "\n",
    "def store_slim_opinion(params, step, sL, s, _input):\n",
    "    \n",
    "    opinion = _input['slim']\n",
    "    \n",
    "    y = 'slim'\n",
    "    \n",
    "    x = opinion\n",
    "    \n",
    "    return (y, x)\n",
    "\n",
    "def store_frosty_opinion(params, step, sL, s, _input):\n",
    "    \n",
    "    opinion = _input['frosty']\n",
    "    \n",
    "    y = 'frosty'\n",
    "    \n",
    "    x = opinion\n",
    "    \n",
    "    return (y, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_slim_suggestion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b69d4349ea34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     { \n\u001b[1;32m      5\u001b[0m         'policies': {\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0;34m'slim'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mget_slim_suggestion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0;34m'frosty'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mget_frosty_suggestion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         },\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_slim_suggestion' is not defined"
     ]
    }
   ],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "# In the Partial State Update Blocks, the user specifies if state update functions will be run in series or in parallel\n",
    "partial_state_update_blocks = [\n",
    "    { \n",
    "        'policies': {\n",
    "            'slim':get_slim_suggestion,\n",
    "            'frosty':get_frosty_suggestion \n",
    "        },\n",
    "        'variables': { # The following state variables will be updated simultaneously\n",
    "            'slim_says': store_slim_suggestion,\n",
    "            'frosty_says': store_frosty_suggestion\n",
    "        }\n",
    "    },\n",
    "    { \n",
    "        'policies': {\n",
    "            'collaborate': collaborate # Improve\n",
    "        },\n",
    "        'variables': { # The following state variables will be updated simultaneously\n",
    "            'transcript': update_transcript,\n",
    "            'slim':store_slim_opinions,\n",
    "            'frosty': store_frosty_opinions\n",
    "        }\n",
    "    }\n",
    "]\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Configuration Parameters\n",
    "Lastly, we define the number of timesteps and the number of Monte Carlo runs of the simulation. These parameters must be passed in a dictionary, in `dict_keys` `T` and `N`, respectively. In our example, we'll run the simulation for 10 timesteps. And because we are dealing with a deterministic system, it makes no sense to have multiple Monte Carlo runs, so we set `N=1`. We'll ignore the `M` key for now and set it to an empty `dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "# Settings of general simulation parameters, unrelated to the system itself\n",
    "# `T` is a range with the number of discrete units of time the simulation will run for;\n",
    "# `N` is the number of times the simulation will be run (Monte Carlo runs)\n",
    "# In this example, we'll run the simulation once (N=1) and its duration will be of 10 timesteps\n",
    "# We'll cover the `M` key in a future article. For now, let's leave it empty\n",
    "simulation_parameters = {\n",
    "    'T': range(10),\n",
    "    'N': 1,\n",
    "    'M': {}\n",
    "}\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "We have defined the state variables of our system and their initial conditions, as well as the state update functions, which have been grouped in a single state update block. We have also specified the parameters of the simulation (number of timesteps and runs). We are now ready to put all those pieces together in a `Configuration` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cadCAD.configuration import Configuration\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "# The configurations above are then packaged into a `Configuration` object\n",
    "config = Configuration(initial_state=initial_conditions, #dict containing variable names and initial values\n",
    "                       partial_state_update_blocks=partial_state_update_blocks, #dict containing state update functions\n",
    "                       sim_config=simulation_parameters #dict containing simulation parameters\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the engine\n",
    "We are now ready to run the engine with the configuration defined above. Instantiate an ExecutionMode, an ExecutionContext and an Executor objects, passing the Configuration object to the latter. Then run the `main()` method of the Executor object, which returns the results of the experiment in the first element of a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from cadCAD.engine import ExecutionMode, ExecutionContext, Executor\n",
    "exec_mode = ExecutionMode()\n",
    "exec_context = ExecutionContext(exec_mode.single_proc)\n",
    "executor = Executor(exec_context, [config]) # Pass the configuration object inside an array\n",
    "raw_result, tensor = executor.main() # The `main()` method returns a tuple; its first elements contains the raw results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the results\n",
    "We can now convert the raw results into a DataFrame for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(raw_result)\n",
    "df.set_index(['run', 'timestep', 'substep'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_About BlockScience_  \n",
    "[BlockScience](http://bit.ly/github_articles_M_1) is a research and engineering firm specialized in complex adaptive systems and applying practical methodologies from engineering design, development and testing to projects in emerging technologies such as blockchain. Follow us on [Medium](http://bit.ly/bsci-medium) or [Twitter](http://bit.ly/bsci-twitter) to stay in touch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
